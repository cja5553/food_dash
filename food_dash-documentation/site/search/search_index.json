{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"food_dash Documentation This food_dash is used to prepare supplemental materials for the project titled \"Using mobile-phone data to assess racial disparities surrounding unhealthy food reliance during the COVID-19 pandemic\". However, it can also be utilized with datasets that draw similarity to the structure of our dataset - that is - monthly datasets aggregated data at a county level, of which we wish to measure the percent of one variable over the percent of the sum of 2 variables [ie x=a/(a+b)]. The goal is to ultimately plot a dashboard visualizing spatio-temporal elements, with the temporal aspect being monthly, and the spatial aspect being at a county level Getting Started Requirements The following packages are required in order to run use food_dash >>> pip install pandas >>> pip install plotly >>> pip install plotly.express >>> pip install datetime >>> pip install urlopen Installation The best way to use this is to simply directly use the code provided in execution.ipynb or create a .py or .ipynb similar to that in the food_dash folder. To do so, simply import the functions using >>> from clean_normalize_data import * >>> from dashboard_creation import * If you'd prefer having the file to execute your code outside the food_dash folder, simply do the following: >>> from food_dash.clean_normalize_data import * >>> from food_dash.dashboard_creation import * Quick Usage Overview Here, we provide a quick overview of how these function works and how you can quickly obtain your spatio-temporal dashboard. Do note that the datasets that are fed into the preparing_data_for_viz function assumes that the columns for your dates are \"date_range_start\" and the columns for the counties are called \"Counties\". To see how each specific function works, refer to the User List for tab. # executing the creation of dashboard here. from clean_normalize_data import * from dashboard_creation import * # This examples creates and saving dashboard for convenience store reliance. # read your first dataset. In this case, it is the semi-aggregated convenience store visits file >>> df1=pd.read_csv(\"data/convenient_store_aggregated_by_county.csv\",index_col=False) # read your second dataset, in this case, the semi-aggregated grocery visits file >>> df2=pd.read_csv(\"data/grocery_aggregated_by_county.csv\",index_col=False) # using the preparing data_for_viz_function to clean the data, normalize it, calculate the \"convenience store reliance index\" and prepare the data for visualization. >>> data=preparing_data_for_viz(df1=data_convenience,df2=data_fast_food >>> unhealthy_col_name=\"convenient store_count\", >>> healthy_col_name=\"supermarket_count\", >>> desired_col_name=\"convenient store reliance\") >>> print(data.head(1)) | Month | County | Convenience store reliance | |--------:|--------:|---------------------------:| | 2019-01 | \"01001\" | 0.23885 | # reading the built-in encoded red-to-green scale >>> cc_scale=green_red_col_scale() # reading the county shapefiles >>> counties=loading_counties_file() #creating and saving the dashboards. >>> show_and_save_plot(save_name=\"dashboard/convenience_store_reliance_plot.html\" attribute_name=\"convenient store reliance\",cc_scale=cc_scale,data=data, counties=counties) The following output is the dashboard produced.","title":"Home"},{"location":"#food_dash-documentation","text":"This food_dash is used to prepare supplemental materials for the project titled \"Using mobile-phone data to assess racial disparities surrounding unhealthy food reliance during the COVID-19 pandemic\". However, it can also be utilized with datasets that draw similarity to the structure of our dataset - that is - monthly datasets aggregated data at a county level, of which we wish to measure the percent of one variable over the percent of the sum of 2 variables [ie x=a/(a+b)]. The goal is to ultimately plot a dashboard visualizing spatio-temporal elements, with the temporal aspect being monthly, and the spatial aspect being at a county level","title":"food_dash Documentation"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#requirements","text":"The following packages are required in order to run use food_dash >>> pip install pandas >>> pip install plotly >>> pip install plotly.express >>> pip install datetime >>> pip install urlopen","title":"Requirements"},{"location":"#installation","text":"The best way to use this is to simply directly use the code provided in execution.ipynb or create a .py or .ipynb similar to that in the food_dash folder. To do so, simply import the functions using >>> from clean_normalize_data import * >>> from dashboard_creation import * If you'd prefer having the file to execute your code outside the food_dash folder, simply do the following: >>> from food_dash.clean_normalize_data import * >>> from food_dash.dashboard_creation import *","title":"Installation"},{"location":"#quick-usage-overview","text":"Here, we provide a quick overview of how these function works and how you can quickly obtain your spatio-temporal dashboard. Do note that the datasets that are fed into the preparing_data_for_viz function assumes that the columns for your dates are \"date_range_start\" and the columns for the counties are called \"Counties\". To see how each specific function works, refer to the User List for tab. # executing the creation of dashboard here. from clean_normalize_data import * from dashboard_creation import * # This examples creates and saving dashboard for convenience store reliance. # read your first dataset. In this case, it is the semi-aggregated convenience store visits file >>> df1=pd.read_csv(\"data/convenient_store_aggregated_by_county.csv\",index_col=False) # read your second dataset, in this case, the semi-aggregated grocery visits file >>> df2=pd.read_csv(\"data/grocery_aggregated_by_county.csv\",index_col=False) # using the preparing data_for_viz_function to clean the data, normalize it, calculate the \"convenience store reliance index\" and prepare the data for visualization. >>> data=preparing_data_for_viz(df1=data_convenience,df2=data_fast_food >>> unhealthy_col_name=\"convenient store_count\", >>> healthy_col_name=\"supermarket_count\", >>> desired_col_name=\"convenient store reliance\") >>> print(data.head(1)) | Month | County | Convenience store reliance | |--------:|--------:|---------------------------:| | 2019-01 | \"01001\" | 0.23885 | # reading the built-in encoded red-to-green scale >>> cc_scale=green_red_col_scale() # reading the county shapefiles >>> counties=loading_counties_file() #creating and saving the dashboards. >>> show_and_save_plot(save_name=\"dashboard/convenience_store_reliance_plot.html\" attribute_name=\"convenient store reliance\",cc_scale=cc_scale,data=data, counties=counties) The following output is the dashboard produced.","title":"Quick Usage Overview"},{"location":"bdd/","text":"Behavior-Driven-Development Title COVID-19 food consumption Dashboard Narrative Feature 1. For clean_normalize_data.py Feature 1.1: function cleaning_data As an: researcher dealing with time and county-level aggregated data, it remains common to witness alot of issues when attempting to wrangle the data. Some very common elements include (A) having county FIPS IDs stored as integers instead of strings, (B) having 4-digit instead of the rightful 5-digit FIPS codes as a result of having FIPS IDs stored as integers (in A), (C) having time stored as a datetime instead string/INT; I want: the correct information (ie data type in A-C above) to be reflected in my data. This includes ensuring that county IDs are accurately reflected as a 5-digit FIPS string, having time stored as a datetime instead string/INT, and ensuring that both the FIPS and datetimes are set as an index as its respective columns. so that: Firstly, it will be easier to deal with the data from all aspects later. This could include making additional computations, running regression models or feeding the data into the dashboard (which in this overarching project is our goal). Failing to use this clean_data function could 'corrupt' and add distress to our latter processes. Feature 1.2 and 1.3: function merging_data and calculating_reliance As an: Researcher looking to compute reliance of unhealthy food choices in spatio-temporal dimensions (ie the spatial aspects are the counties and the temporal aspects are the monthly-level time-frames), these two sub-features are targetted towards helping me merge the data before calculating the appropriate reliances for unhealthy food-consumption. I want: to have features that will allow me to 'spit-out' a dataframe that contains my \"unhealthy reliance\" metric based on my specified attribute of interest. To do so, in feature 1.2 (merging_data) we merge the dataframes containing the attributed of interest and in feature 1.3 (calculating_reliance) we feed the merged dataframes with the respective attributes deemed of \"unhealthy reliance\" such that feature 1.3 is able to automatically return to us a normalized dataframe containing our \"unhealthy reliance\" metric of interest. so that: we can obtain (A) a normalized dataframe with our \"unhealthy reliance\" metric based on our specified unhealthy attributes and (B) using this normalized dataframe, we can feed it into the spatio_temporal_dash.py file. The goal of this file is to return to us a spatio-temporal dashboard. Feature 1.4: function saved_normalized As an: Researcher dealing with large amounts of data across multiple spatial and temporal dimensions, of which, at this feature point, we have conducted alot of wrangling work. Thus, having this function that will save our normalized data in a simple and efficient manner for easy retrieval is critical. I want: to efficiently normalize data that is of importance to us and save them in a streamlined and efficient manner so that: I could understand the workflow of my (A) research progress and (B) extract these datasets whenever it is needed in an efficient manner. (so I wouldn't have to redo the entire workflow again). Feature 1.5: function preparing_data_for_viz As an: Researcher whose ultimate goal is to have a dataframe which could be directly read into spatio_temporal_dash.py (this function will spit out a spatio-temporal dashboard) I want: have functions, which calls the above-mentioned functions, and produces a dataframe with my \"unhealthy reliance\" metric calculated and my spatial (ie county-level FIPS ID) and temporal (ie monthly datetime) clearly identified. so that: We could fun this function, of which the output could simply be fed into spatio_temporal_dash.py (this function will spit out a spatio-temporal dashboard), in a manner whereby spatio_temporal_dash.py is solely focused on the visual aspects of the dashboard, instead of trying to wrangly through untidy and uncleaned datasets while trying to process a spatio-temporal dashboard, which can prove to be tricky. Feature 2. For dashboard_creation.py As an Researcher looking to generate a spatial temporal dashboard for my readers and the journal editors I want To be able to generate a function that 'spits out' a spatial-temporal dashboard with ease (ie one line of code which simply calls the function) so that the spatial temporal dashboard is done in a simple and visually appeasing manner and individuals wishing to do something similar (ie replicate the dashboard with the same or a different dataset) could do so by simply calling this function. Acceptance criteria Feature 1. For clean_normalize_data.py Given That a researcher has a numerous US census county-level dataset which takes place across numerous monthly scales. When a typical CSV file from most conventional data sources (including those of the .gov websites) produces (A) extremely undesirable columns (for example the 5-digit FIPS code is an INT rather than a string and some of the '0's at the fore have been removed as a result OR time is saved as a STR instead of DATETIME), AND The researcher wants to calculate a proportional metric that requires divisional computation of attributes scattered across distinct datasets. Then The researcher could utilize and call the relevant functions from clean_normalize_data.py to perform cleaning and saved the normalized dataset, of which it will be expected to be used for futuristic purposes, of which in my case, it is to prepare a 'cleaned' and 'tidy' dataset for a dashboard. Other cases could include preparing it for spatial or time-series regression/panel models. Feature 2. For spatio_temporal_dash.py Given A researcher wants to proudce a spatial-temporal dashboard with ease When he/she has a cleaned/tidy dataset with the appropriate ID column linking it to the spatial elements (such as county/cbg/state) and a datetime column linking it with the temporal elements of the dataset in which he/she intends to visualize. Then This function can be called upon (of which the dataset is fed into) to produce and 'spit-out' a spatio-temporal dashbaord with ease, thus saving time and energy needed to visualize the multi-dimensional aspects of spatial-temporal data.","title":"Behavior Driven Development"},{"location":"bdd/#behavior-driven-development","text":"","title":"Behavior-Driven-Development"},{"location":"bdd/#title","text":"COVID-19 food consumption Dashboard","title":"Title"},{"location":"bdd/#narrative","text":"","title":"Narrative"},{"location":"bdd/#feature-1-for-clean_normalize_datapy","text":"","title":"Feature 1. For clean_normalize_data.py"},{"location":"bdd/#feature-11-function-cleaning_data","text":"","title":"Feature 1.1: function cleaning_data"},{"location":"bdd/#as-an","text":"researcher dealing with time and county-level aggregated data, it remains common to witness alot of issues when attempting to wrangle the data. Some very common elements include (A) having county FIPS IDs stored as integers instead of strings, (B) having 4-digit instead of the rightful 5-digit FIPS codes as a result of having FIPS IDs stored as integers (in A), (C) having time stored as a datetime instead string/INT;","title":"As an:"},{"location":"bdd/#i-want","text":"the correct information (ie data type in A-C above) to be reflected in my data. This includes ensuring that county IDs are accurately reflected as a 5-digit FIPS string, having time stored as a datetime instead string/INT, and ensuring that both the FIPS and datetimes are set as an index as its respective columns.","title":"I want:"},{"location":"bdd/#so-that","text":"Firstly, it will be easier to deal with the data from all aspects later. This could include making additional computations, running regression models or feeding the data into the dashboard (which in this overarching project is our goal). Failing to use this clean_data function could 'corrupt' and add distress to our latter processes.","title":"so that:"},{"location":"bdd/#feature-12-and-13-function-merging_data-and-calculating_reliance","text":"","title":"Feature 1.2 and 1.3: function merging_data and calculating_reliance"},{"location":"bdd/#as-an_1","text":"Researcher looking to compute reliance of unhealthy food choices in spatio-temporal dimensions (ie the spatial aspects are the counties and the temporal aspects are the monthly-level time-frames), these two sub-features are targetted towards helping me merge the data before calculating the appropriate reliances for unhealthy food-consumption.","title":"As an:"},{"location":"bdd/#i-want_1","text":"to have features that will allow me to 'spit-out' a dataframe that contains my \"unhealthy reliance\" metric based on my specified attribute of interest. To do so, in feature 1.2 (merging_data) we merge the dataframes containing the attributed of interest and in feature 1.3 (calculating_reliance) we feed the merged dataframes with the respective attributes deemed of \"unhealthy reliance\" such that feature 1.3 is able to automatically return to us a normalized dataframe containing our \"unhealthy reliance\" metric of interest.","title":"I want:"},{"location":"bdd/#so-that_1","text":"we can obtain (A) a normalized dataframe with our \"unhealthy reliance\" metric based on our specified unhealthy attributes and (B) using this normalized dataframe, we can feed it into the spatio_temporal_dash.py file. The goal of this file is to return to us a spatio-temporal dashboard.","title":"so that:"},{"location":"bdd/#feature-14-function-saved_normalized","text":"","title":"Feature 1.4: function saved_normalized"},{"location":"bdd/#as-an_2","text":"Researcher dealing with large amounts of data across multiple spatial and temporal dimensions, of which, at this feature point, we have conducted alot of wrangling work. Thus, having this function that will save our normalized data in a simple and efficient manner for easy retrieval is critical.","title":"As an:"},{"location":"bdd/#i-want_2","text":"to efficiently normalize data that is of importance to us and save them in a streamlined and efficient manner","title":"I want:"},{"location":"bdd/#so-that_2","text":"I could understand the workflow of my (A) research progress and (B) extract these datasets whenever it is needed in an efficient manner. (so I wouldn't have to redo the entire workflow again).","title":"so that:"},{"location":"bdd/#feature-15-function-preparing_data_for_viz","text":"","title":"Feature 1.5: function preparing_data_for_viz"},{"location":"bdd/#as-an_3","text":"Researcher whose ultimate goal is to have a dataframe which could be directly read into spatio_temporal_dash.py (this function will spit out a spatio-temporal dashboard)","title":"As an:"},{"location":"bdd/#i-want_3","text":"have functions, which calls the above-mentioned functions, and produces a dataframe with my \"unhealthy reliance\" metric calculated and my spatial (ie county-level FIPS ID) and temporal (ie monthly datetime) clearly identified.","title":"I want:"},{"location":"bdd/#so-that_3","text":"We could fun this function, of which the output could simply be fed into spatio_temporal_dash.py (this function will spit out a spatio-temporal dashboard), in a manner whereby spatio_temporal_dash.py is solely focused on the visual aspects of the dashboard, instead of trying to wrangly through untidy and uncleaned datasets while trying to process a spatio-temporal dashboard, which can prove to be tricky.","title":"so that:"},{"location":"bdd/#feature-2-for-dashboard_creationpy","text":"","title":"Feature 2. For dashboard_creation.py"},{"location":"bdd/#as-an_4","text":"Researcher looking to generate a spatial temporal dashboard for my readers and the journal editors","title":"As an"},{"location":"bdd/#i-want_4","text":"To be able to generate a function that 'spits out' a spatial-temporal dashboard with ease (ie one line of code which simply calls the function)","title":"I want"},{"location":"bdd/#so-that_4","text":"the spatial temporal dashboard is done in a simple and visually appeasing manner and individuals wishing to do something similar (ie replicate the dashboard with the same or a different dataset) could do so by simply calling this function.","title":"so that"},{"location":"bdd/#acceptance-criteria","text":"","title":"Acceptance criteria"},{"location":"bdd/#feature-1-for-clean_normalize_datapy_1","text":"","title":"Feature 1. For clean_normalize_data.py"},{"location":"bdd/#given","text":"That a researcher has a numerous US census county-level dataset which takes place across numerous monthly scales.","title":"Given"},{"location":"bdd/#when","text":"a typical CSV file from most conventional data sources (including those of the .gov websites) produces (A) extremely undesirable columns (for example the 5-digit FIPS code is an INT rather than a string and some of the '0's at the fore have been removed as a result OR time is saved as a STR instead of DATETIME), AND The researcher wants to calculate a proportional metric that requires divisional computation of attributes scattered across distinct datasets.","title":"When"},{"location":"bdd/#then","text":"The researcher could utilize and call the relevant functions from clean_normalize_data.py to perform cleaning and saved the normalized dataset, of which it will be expected to be used for futuristic purposes, of which in my case, it is to prepare a 'cleaned' and 'tidy' dataset for a dashboard. Other cases could include preparing it for spatial or time-series regression/panel models.","title":"Then"},{"location":"bdd/#feature-2-for-spatio_temporal_dashpy","text":"","title":"Feature 2. For spatio_temporal_dash.py"},{"location":"bdd/#given_1","text":"A researcher wants to proudce a spatial-temporal dashboard with ease","title":"Given"},{"location":"bdd/#when_1","text":"he/she has a cleaned/tidy dataset with the appropriate ID column linking it to the spatial elements (such as county/cbg/state) and a datetime column linking it with the temporal elements of the dataset in which he/she intends to visualize.","title":"When"},{"location":"bdd/#then_1","text":"This function can be called upon (of which the dataset is fed into) to produce and 'spit-out' a spatio-temporal dashbaord with ease, thus saving time and energy needed to visualize the multi-dimensional aspects of spatial-temporal data.","title":"Then"},{"location":"userGuide/","text":"User Guide clean_normalize_data clean_normalize_data.cleaning_data(data) Reads the data with the aggregated visit counts and converts the columns into its neccessary data type. Returns the DataFrame with columns converted to its appropriate types and inappropriate values fixed. This includes converting counties to strings, filling them up with 0's if they are 4-digits long, and converting dates into datetimes. data ( pandas.DataFrame ) - this arguement takes a dataFrame with at least 3 columns (1) the aggregated visitation counts for each county on each month, (2) the county FIPS code, and (3) the month (or date). It assumes that dates are titled date_range_start , and counties are titled County . clean_normalize_data.merging_data(df1,df2) Merges the specified 2 dataFrames based on Month and County set as the index of the dataFrames. Returns 1 merged DataFrame containing attributes from both the DataFrames, based on each Month on each County df1 ( pandas.dataFrame ) - this arguement takes a first dataFrame with at least 3 columns (1) the aggregated visitation counts for each county on each month, (2) the county FIPS code, and (3) the month (or date). It assumes that the columns contains the appropriate data types. Feed it into the cleaning_data if the columns do not contain the appropriate data types. df2 ( pandas.dataFrame ) - same as df1 clean_normalize_data.calculating_reliance(data,unhealthy_col_name,healthy_col_name,desired_col_name) Takes a DataFrame with one column as the unhealthy_col_name and another as the healthy_col_name . Returns DataFrame with the computed unhealthy reliance index for each specified County on each Month - calculated by unhealthy_col_name /( unhealthy_col_name + healthy_col_name ). data ( pandas.dataFrame ) - takes a dataFrame which contains the following columns: County , Month , the specified unhealthy_col_name , and unhealthy_col_name unhealthy_col_name ( str ) - Name of the column of which we wish to identify as our unhealthy element, of which we will calculate our \"unhealthy reliance index\" by. healthy_col_name ( str ) - Name of the column of which we wish to identify as our healthy element. desired_col_name ( str ) - Column Name of our specified \"unhealthy reliance index\", to which the data frame of our output will reflect clean_normalize_data.preparing_data_for_viz(df1,df2,unhealthy_col_name,healthy_col_name,desired_col_name) Combines all the above functions together (ie calculating_reliance() , merging_data() , cleaning_data() ). Returns an output of a DataFrame that is ready to be fed into the functions used to construct the Dashboard. df1 ( pandas.dataFrame ): Refer to merging_data above df2 ( pandas.dataFrame ): Refer to merging_data above unhealthy_col_name ( str ): Refer to preparing_data_for_viz above. healthy_col_name ( str ): Refer to preparing_data_for_viz above. desired_col_name ( str ): Refer to preparing_data_for_viz above. dashboard_creation dashboard_creation.loading_counties_file() Returns the county shapefiles. dashboard_creation.green_red_col_scale() Returns the scale of which we use to visualize our dashboard. The scale ranges from 0 to 1, with 0 being dark-red in color to 1 being green in color. Here 0 is assumed to be the worst and 1 is assumed to be the best. dashboard_creation.show_and_save_plot(save_name,attribute_name,cc_scale,data, counties) Returns a Dashboard and saves the dashboard based on the assigned save_name save_name ( str ): Name and folder location of where we wish to save our Dashboard attribute_name ( str ): Column of the variable of interest that we wish to spatio-temporally visualize in our dashboard. cc_scale ( dict ): dictionary of the color scales and its corresponding values which we wish to have displayed on our dashboard. data ( pandas.DataFrame ): dataFrame of our data to be used to construct our dashboard. counties ( json shpfile ): json shapefile of our counties.","title":"User Guide"},{"location":"userGuide/#user-guide","text":"","title":"User Guide"},{"location":"userGuide/#clean_normalize_data","text":"","title":"clean_normalize_data"},{"location":"userGuide/#clean_normalize_datacleaning_datadata","text":"Reads the data with the aggregated visit counts and converts the columns into its neccessary data type. Returns the DataFrame with columns converted to its appropriate types and inappropriate values fixed. This includes converting counties to strings, filling them up with 0's if they are 4-digits long, and converting dates into datetimes. data ( pandas.DataFrame ) - this arguement takes a dataFrame with at least 3 columns (1) the aggregated visitation counts for each county on each month, (2) the county FIPS code, and (3) the month (or date). It assumes that dates are titled date_range_start , and counties are titled County .","title":"clean_normalize_data.cleaning_data(data)"},{"location":"userGuide/#clean_normalize_datamerging_datadf1df2","text":"Merges the specified 2 dataFrames based on Month and County set as the index of the dataFrames. Returns 1 merged DataFrame containing attributes from both the DataFrames, based on each Month on each County df1 ( pandas.dataFrame ) - this arguement takes a first dataFrame with at least 3 columns (1) the aggregated visitation counts for each county on each month, (2) the county FIPS code, and (3) the month (or date). It assumes that the columns contains the appropriate data types. Feed it into the cleaning_data if the columns do not contain the appropriate data types. df2 ( pandas.dataFrame ) - same as df1","title":"clean_normalize_data.merging_data(df1,df2)"},{"location":"userGuide/#clean_normalize_datacalculating_reliancedataunhealthy_col_namehealthy_col_namedesired_col_name","text":"Takes a DataFrame with one column as the unhealthy_col_name and another as the healthy_col_name . Returns DataFrame with the computed unhealthy reliance index for each specified County on each Month - calculated by unhealthy_col_name /( unhealthy_col_name + healthy_col_name ). data ( pandas.dataFrame ) - takes a dataFrame which contains the following columns: County , Month , the specified unhealthy_col_name , and unhealthy_col_name unhealthy_col_name ( str ) - Name of the column of which we wish to identify as our unhealthy element, of which we will calculate our \"unhealthy reliance index\" by. healthy_col_name ( str ) - Name of the column of which we wish to identify as our healthy element. desired_col_name ( str ) - Column Name of our specified \"unhealthy reliance index\", to which the data frame of our output will reflect","title":"clean_normalize_data.calculating_reliance(data,unhealthy_col_name,healthy_col_name,desired_col_name)"},{"location":"userGuide/#clean_normalize_datapreparing_data_for_vizdf1df2unhealthy_col_namehealthy_col_namedesired_col_name","text":"Combines all the above functions together (ie calculating_reliance() , merging_data() , cleaning_data() ). Returns an output of a DataFrame that is ready to be fed into the functions used to construct the Dashboard. df1 ( pandas.dataFrame ): Refer to merging_data above df2 ( pandas.dataFrame ): Refer to merging_data above unhealthy_col_name ( str ): Refer to preparing_data_for_viz above. healthy_col_name ( str ): Refer to preparing_data_for_viz above. desired_col_name ( str ): Refer to preparing_data_for_viz above.","title":"clean_normalize_data.preparing_data_for_viz(df1,df2,unhealthy_col_name,healthy_col_name,desired_col_name)"},{"location":"userGuide/#dashboard_creation","text":"","title":"dashboard_creation"},{"location":"userGuide/#dashboard_creationloading_counties_file","text":"Returns the county shapefiles.","title":"dashboard_creation.loading_counties_file()"},{"location":"userGuide/#dashboard_creationgreen_red_col_scale","text":"Returns the scale of which we use to visualize our dashboard. The scale ranges from 0 to 1, with 0 being dark-red in color to 1 being green in color. Here 0 is assumed to be the worst and 1 is assumed to be the best.","title":"dashboard_creation.green_red_col_scale()"},{"location":"userGuide/#dashboard_creationshow_and_save_plotsave_nameattribute_namecc_scaledata-counties","text":"Returns a Dashboard and saves the dashboard based on the assigned save_name save_name ( str ): Name and folder location of where we wish to save our Dashboard attribute_name ( str ): Column of the variable of interest that we wish to spatio-temporally visualize in our dashboard. cc_scale ( dict ): dictionary of the color scales and its corresponding values which we wish to have displayed on our dashboard. data ( pandas.DataFrame ): dataFrame of our data to be used to construct our dashboard. counties ( json shpfile ): json shapefile of our counties.","title":"dashboard_creation.show_and_save_plot(save_name,attribute_name,cc_scale,data, counties)"}]}